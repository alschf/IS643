{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender system project:\n",
    "Code based on a tutorial by Agnes Johannsdottir using MovieLens data set.  \n",
    "Here I use the million song data set from Kaggle.\n",
    "\n",
    "P1.  Problems include that the similiarity matrix is extremely sparse (with 163000 users for instance) and the kernel crashes when generating it.  I addressed these issues by removing users and items with fewer than 30 entries.  This still provides a very sparse matrix (99.8%), and resulted in a RMSE of ~7 with either a item-item collaboration or user-item collaboration. Note that this RMSE is worse than the standard deviation! \n",
    "\n",
    "Limiting the data set to heavily reviewed songs and 52 active users provided a 92% sparse matrix; now the RMSE is better than the std, although not by a large amount.  Now the RMSE is 6~7 and the std is ~8.1 (note the RMSE has some variance due to how the makeup of the test and training set).\n",
    "\n",
    "P2.  Here I've written a function with generates a similarity matrix identical to that of the sklearn function.\n",
    "\n",
    "P3.  Here I've assigned all 'ratings' greater than 5 to 5, so that the scale is fixed.  This does not help though, and the std is still less than the RMSE. (note that in this part I did not limit users and items to minimize the sparsness of the matrix).\n",
    "\n",
    "P4.  Imputed with user averages.  This didn't improve the RMSE by much.\n",
    "\n",
    "conclusions: Overall, I find that the recommender system performs at best equal to the 'average' value, and so is providing little value.  I think the next steps are to find a better way to impute missing values, perhaps by taking item averages and then imputing with user bias...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P1: using sklearn pairwise_distance to generate the similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"/Users/alexandersatz/Documents/Cuny/IS643_recommenderSys/project1/kaggle_visible_evaluation_triplets.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line[:-1]\n",
    "        line =line.split('\\t')\n",
    "        data.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fd50c4007b68a3737fe052d5a4f78ce8aa117f3d', 'SOBONKR12A58A7A7E0', '1'],\n",
       " ['fd50c4007b68a3737fe052d5a4f78ce8aa117f3d', 'SOEGIYH12A6D4FC0E3', '1'],\n",
       " ['fd50c4007b68a3737fe052d5a4f78ce8aa117f3d', 'SOFLJQZ12A6D4FADA6', '1'],\n",
       " ['fd50c4007b68a3737fe052d5a4f78ce8aa117f3d', 'SOHTKMO12AB01843B0', '1'],\n",
       " ['fd50c4007b68a3737fe052d5a4f78ce8aa117f3d', 'SODQZCY12A6D4F9D11', '1'],\n",
       " ['fd50c4007b68a3737fe052d5a4f78ce8aa117f3d', 'SOXLOQG12AF72A2D55', '1'],\n",
       " ['d7083f5e1d50c264277d624340edaaf3dc16095b', 'SOUVUHC12A67020E3B', '1'],\n",
       " ['d7083f5e1d50c264277d624340edaaf3dc16095b', 'SOUQERE12A58A75633', '1'],\n",
       " ['d7083f5e1d50c264277d624340edaaf3dc16095b', 'SOIPJAX12A8C141A2D', '1'],\n",
       " ['d7083f5e1d50c264277d624340edaaf3dc16095b', 'SOEFCDJ12AB0185FA0', '2']]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "de_item = defaultdict(int)\n",
    "for row in data:\n",
    "    de_item[row[1]] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here I'm removing items with few reviews.\n",
    "d_itemGTE5 = {}\n",
    "for key, value in de_item.iteritems():\n",
    "    if value > 100:\n",
    "        d_itemGTE5[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1758\n",
      "163206\n"
     ]
    }
   ],
   "source": [
    "print len(d_itemGTE5)\n",
    "print len(de_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shortdata = []\n",
    "for x in data:\n",
    "    if x[1] in d_itemGTE5:\n",
    "        shortdata.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "de_user = defaultdict(int)\n",
    "for row in shortdata:\n",
    "    de_user[row[0]] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here I'm removing users with few reviews.\n",
    "d_usersGTE10 = {}\n",
    "for key, value in de_user.iteritems():\n",
    "    if value > 30:\n",
    "        d_usersGTE10[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shortdata2 = []\n",
    "for x in shortdata:\n",
    "    if x[0] in d_usersGTE10:\n",
    "        shortdata2.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shortdata = shortdata2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1450933\n",
      "1998\n"
     ]
    }
   ],
   "source": [
    "## at atleast 20 occurances of each, this leaves 92219 rows.\n",
    "print(len(data))\n",
    "print(len(shortdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data from https://www.kaggle.com/c/msdchallenge/data\n",
    "#the number of times a user listens to a song is tracked\n",
    "\n",
    "#here 'rating' is number of listens to the song.\n",
    "header = ['user_id', 'item_id', 'rating']\n",
    "df = pd.DataFrame(shortdata, columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1998, 3)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37a309640004e11829ff173e9c3f9fbd2e07ab29</td>\n",
       "      <td>SOGTQNI12AB0184A5C</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37a309640004e11829ff173e9c3f9fbd2e07ab29</td>\n",
       "      <td>SOPUCYA12A8C13A694</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37a309640004e11829ff173e9c3f9fbd2e07ab29</td>\n",
       "      <td>SOELOOM12AB017DB4C</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37a309640004e11829ff173e9c3f9fbd2e07ab29</td>\n",
       "      <td>SONYKOW12AB01849C9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37a309640004e11829ff173e9c3f9fbd2e07ab29</td>\n",
       "      <td>SOOFTNW12AB017DB3E</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    user_id             item_id rating\n",
       "0  37a309640004e11829ff173e9c3f9fbd2e07ab29  SOGTQNI12AB0184A5C      6\n",
       "1  37a309640004e11829ff173e9c3f9fbd2e07ab29  SOPUCYA12A8C13A694     39\n",
       "2  37a309640004e11829ff173e9c3f9fbd2e07ab29  SOELOOM12AB017DB4C     62\n",
       "3  37a309640004e11829ff173e9c3f9fbd2e07ab29  SONYKOW12AB01849C9      7\n",
       "4  37a309640004e11829ff173e9c3f9fbd2e07ab29  SOOFTNW12AB017DB3E      1"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## So each row is a userID, an item_id, and a rating\n",
    "##df_o = df_o.head(100000) this causes the matrix to be too sparse\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_users, d_items = {}, {}\n",
    "c = 0\n",
    "for x in arrayusers:\n",
    "    c +=1\n",
    "    d_users[x] = c \n",
    "c = 0\n",
    "for x in arrayitems:\n",
    "    c +=1\n",
    "    d_items[x] = c \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#I need to convert each user_id to an int, and each item_id to an int.\n",
    "arrayusers = df.user_id.unique()\n",
    "arrayitems = df.item_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of users left\n",
    "len(d_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    df.set_value(index, 'user_id', d_users[row['user_id']])\n",
    "    df.set_value(index, 'item_id', d_items[row['item_id']])\n",
    "    #df[index]['user_id'] = d_users[row['user_id']]\n",
    "    #df[index]['item_id'] = d_items[row['item_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id item_id rating\n",
       "0       1       1      6\n",
       "1       1       2     39\n",
       "2       1       3     62\n",
       "3       1       4      7\n",
       "4       1       5      1"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users = 58 | Number of items = 423\n"
     ]
    }
   ],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "print 'Number of users = ' + str(n_users) + ' | Number of items = ' + str(n_items)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation as cv\n",
    "train_data, test_data = cv.train_test_split(df, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     user_id item_id rating\n",
      "190        6     139      1\n",
      "654       20     123      6\n",
      "1461      43     138      1\n",
      "638       19     271      1\n",
      "660       20      86      1\n",
      "1104      33      59      1\n",
      "1381      41     121      2\n",
      "1556      46      62      2\n",
      "838       25     272      1\n",
      "738       22       4      3\n",
      "(1498, 3)\n",
      "(1998, 3)\n"
     ]
    }
   ],
   "source": [
    "print train_data.head(10)\n",
    "print train_data.shape\n",
    "print df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create two user-item matrices, one for training and another for testing\n",
    "## row number = user_id-1, column number is item_id-1, and value is the rating.\n",
    "## so each row has all ratings for every movie for that user (else a zero)\n",
    "train_data_matrix = np.zeros((n_users, n_items))\n",
    "for line in train_data.itertuples():\n",
    "    train_data_matrix[line[1]-1, line[2]-1] = line[3]  \n",
    "\n",
    "test_data_matrix = np.zeros((n_users, n_items))\n",
    "for line in test_data.itertuples():\n",
    "    test_data_matrix[line[1]-1, line[2]-1] = line[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 423)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##943 users\n",
    "train_data_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "user_similarity = pairwise_distances(train_data_matrix, metric='cosine')\n",
    "item_similarity = pairwise_distances(train_data_matrix.T, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 58)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## a 2Dmatrix with similarity score 0-1 between all users\n",
    "user_similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(423, 423)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## a 2D matrix with similarity score 0-1 between all items\n",
    "## note that when a rating is 0, it gives a 0 score automatically...\n",
    "item_similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict(train_data_matrix, user_similarity, type='user')\n",
    "def predict(ratings, similarity, type='user'):\n",
    "    if type == 'user':\n",
    "        mean_user_rating = ratings.mean(axis=1)  # array with mean for each user\n",
    "        ratings_diff = (ratings - mean_user_rating[:, np.newaxis]) \n",
    "        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif type == 'item':\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])     \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_prediction = predict(train_data_matrix, item_similarity, type='item')\n",
    "user_prediction = predict(train_data_matrix, user_similarity, type='user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "def rmse(prediction, ground_truth):\n",
    "    prediction = prediction[ground_truth.nonzero()].flatten() \n",
    "    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(prediction, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF RMSE: 5.82894997334\n",
      "Item-based CF RMSE: 5.92528052976\n"
     ]
    }
   ],
   "source": [
    "print 'User-based CF RMSE: ' + str(rmse(user_prediction, test_data_matrix))\n",
    "print 'Item-based CF RMSE: ' + str(rmse(item_prediction, test_data_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sparsity level is 91.9%\n"
     ]
    }
   ],
   "source": [
    "sparsity=round(1.0-len(df)/float(n_users*n_items),3)\n",
    "print 'The sparsity level is ' +  str(sparsity*100) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF MSE: 6.07667169796\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "#get SVD components from train matrix. Choose k.\n",
    "u, s, vt = svds(train_data_matrix, k = 20)\n",
    "s_diag_matrix=np.diag(s)\n",
    "X_pred = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "print 'User-based CF MSE: ' + str(rmse(X_pred, test_data_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.341333292967498"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['rating']] = df[['rating']].apply(pd.to_numeric)\n",
    "np.std((df['rating']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P2:  Handcoded similarity function\n",
    "\n",
    "I use a small test matrix to test my handcoded function with the sklearn function and show the two functions to provide equivalent results.  Granted, my function is much slower with large matrix (brutally slow).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 0, 0],\n",
       "       [0, 0, 0, 2],\n",
       "       [2, 4, 4, 4]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a test matrix\n",
    "rows  = 3\n",
    "cols = 4\n",
    "low = 0\n",
    "high = 5\n",
    "step = 2\n",
    "\n",
    "test = np.random.choice([x for x in xrange(low,high,step)],rows*cols)\n",
    "test.resize(rows,cols)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genSimMatrix(mat):\n",
    "    newmat = np.matlib.zeros((mat.shape[0],mat.shape[0]))\n",
    "    for x in range(0, mat.shape[0]):\n",
    "        for y in range(0, mat.shape[0]):\n",
    "            sim =  np.dot(mat[x], mat[y,].T)/(np.dot(mat[x], mat[x,].T)*np.dot(mat[y], mat[y,].T))**.5\n",
    "            newmat[x,y] = 1-sim\n",
    "    return newmat\n",
    "        \n",
    "        \n",
    "\n",
    "#user_similarity = pairwise_distances(train_data_matrix, metric='cosine')\n",
    "#item_similarity = pairwise_distances(train_data_matrix.T, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy.matlib\n",
    "handcoded = genSimMatrix(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.        ,  1.        ,  0.41165159],\n",
       "        [ 1.        ,  0.        ,  0.4452998 ],\n",
       "        [ 0.41165159,  0.4452998 ,  0.        ]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the handcoded similarity matrix is below.  ## note that larger numbers mean MORE different.\n",
    "handcoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "usingsci = pairwise_distances(test, metric='cosine', Y = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.22044605e-16,   1.00000000e+00,   4.11651595e-01],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   4.45299804e-01],\n",
       "       [  4.11651595e-01,   4.45299804e-01,   0.00000000e+00]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the sklearn matrix is below.\n",
    "usingsci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3:  set all 'ratings' greater than 5 to 5\n",
    "\n",
    "The scale for 'ratings' in confusing, as one can listen to a song as many times as one like, akin to ranking it as high as one desires.  Here I've set the limit to 5 listens.  This makes understanding the RMSE easier.  \n",
    "\n",
    "Note that the RMSE is ~2.6 while the standard deviation is 1.6.  And so our collaborative recommender is worse than just taking the average (unless we take efforts to reduce how sparse the data is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data from https://www.kaggle.com/c/msdchallenge/data\n",
    "#the number of times a user listens to a song is tracked\n",
    "\n",
    "#here 'rating' is number of listens to the song.\n",
    "header = ['user_id', 'item_id', 'rating']\n",
    "df = pd.DataFrame(shortdata, columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max listen is now 5, which represents 'liking' a song\n",
    "for index, row in df.iterrows():\n",
    "    if int(row['rating']) > 5:\n",
    "        row['rating'] = 5\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF RMSE: 2.64583489919\n",
      "Item-based CF RMSE: 2.65285747954\n"
     ]
    }
   ],
   "source": [
    "arrayusers = df.user_id.unique()\n",
    "arrayitems = df.item_id.unique()\n",
    "d_users, d_items = {}, {}\n",
    "c = 0\n",
    "for x in arrayusers:\n",
    "    c +=1\n",
    "    d_users[x] = c \n",
    "c = 0\n",
    "for x in arrayitems:\n",
    "    c +=1\n",
    "    d_items[x] = c \n",
    "for index, row in df.iterrows():\n",
    "    df.set_value(index, 'user_id', d_users[row['user_id']])\n",
    "    df.set_value(index, 'item_id', d_items[row['item_id']])\n",
    "from sklearn import cross_validation as cv\n",
    "train_data, test_data = cv.train_test_split(df, test_size=0.25)\n",
    "train_data_matrix = np.zeros((n_users, n_items))\n",
    "for line in train_data.itertuples():\n",
    "    train_data_matrix[line[1]-1, line[2]-1] = line[3]  \n",
    "\n",
    "test_data_matrix = np.zeros((n_users, n_items))\n",
    "for line in test_data.itertuples():\n",
    "    test_data_matrix[line[1]-1, line[2]-1] = line[3]\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "user_similarity = pairwise_distances(train_data_matrix, metric='cosine')\n",
    "item_similarity = pairwise_distances(train_data_matrix.T, metric='cosine')\n",
    "item_prediction = predict(train_data_matrix, item_similarity, type='item')\n",
    "user_prediction = predict(train_data_matrix, user_similarity, type='user')\n",
    "print 'User-based CF RMSE: ' + str(rmse(user_prediction, test_data_matrix))\n",
    "print 'Item-based CF RMSE: ' + str(rmse(item_prediction, test_data_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF MSE: 2.63190546368\n"
     ]
    }
   ],
   "source": [
    "#get SVD components from train matrix. Choose k.\n",
    "u, s, vt = svds(train_data_matrix, k = 20)\n",
    "s_diag_matrix=np.diag(s)\n",
    "X_pred = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "print 'User-based CF MSE: ' + str(rmse(X_pred, test_data_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[['rating']] = df[['rating']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5516252476670083"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std((df['rating']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P4: impute with user averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1758\n",
      "163206\n",
      "User-based CF RMSE: 6.805159312\n",
      "Item-based CF RMSE: 6.86238598893\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "with open(\"/Users/alexandersatz/Documents/Cuny/IS643_recommenderSys/project1/kaggle_visible_evaluation_triplets.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line[:-1]\n",
    "        line =line.split('\\t')\n",
    "        data.append(line)\n",
    "\n",
    "de_item = defaultdict(int)\n",
    "for row in data:\n",
    "    de_item[row[1]] += 1\n",
    "    \n",
    "\n",
    "\n",
    "# In[201]:\n",
    "\n",
    "#Here I'm removing items with few reviews.\n",
    "d_itemGTE5 = {}\n",
    "for key, value in de_item.iteritems():\n",
    "    if value > 100:\n",
    "        d_itemGTE5[key] = 1\n",
    "\n",
    "\n",
    "# In[202]:\n",
    "\n",
    "print len(d_itemGTE5)\n",
    "print len(de_item)\n",
    "\n",
    "\n",
    "# In[203]:\n",
    "\n",
    "shortdata = []\n",
    "for x in data:\n",
    "    if x[1] in d_itemGTE5:\n",
    "        shortdata.append(x)\n",
    "\n",
    "\n",
    "# In[204]:\n",
    "\n",
    "de_user = defaultdict(int)\n",
    "for row in shortdata:\n",
    "    de_user[row[0]] += 1\n",
    "\n",
    "d_usersGTE10 = {}\n",
    "for key, value in de_user.iteritems():\n",
    "    if value > 30:\n",
    "        d_usersGTE10[key] = 1\n",
    "\n",
    "\n",
    "# In[206]:\n",
    "\n",
    "shortdata2 = []\n",
    "for x in shortdata:\n",
    "    if x[0] in d_usersGTE10:\n",
    "        shortdata2.append(x)\n",
    "        \n",
    "\n",
    "shortdata = shortdata2\n",
    "\n",
    "\n",
    "header = ['user_id', 'item_id', 'rating']\n",
    "df = pd.DataFrame(shortdata, columns=header)\n",
    "\n",
    "\n",
    "\n",
    "arrayusers = df.user_id.unique()\n",
    "arrayitems = df.item_id.unique()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "d_users, d_items = {}, {}\n",
    "c = 0\n",
    "for x in arrayusers:\n",
    "    c +=1\n",
    "    d_users[x] = c \n",
    "c = 0\n",
    "for x in arrayitems:\n",
    "    c +=1\n",
    "    d_items[x] = c \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    df.set_value(index, 'user_id', d_users[row['user_id']])\n",
    "    df.set_value(index, 'item_id', d_items[row['item_id']])\n",
    "\n",
    "\n",
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    " \n",
    "\n",
    "\n",
    "from sklearn import cross_validation as cv\n",
    "train_data, test_data = cv.train_test_split(df, test_size=0.25)\n",
    "\n",
    "\n",
    "\n",
    "train_data_matrix = np.zeros((n_users, n_items))\n",
    "for line in train_data.itertuples():\n",
    "    train_data_matrix[line[1]-1, line[2]-1] = line[3]  \n",
    "\n",
    "test_data_matrix = np.zeros((n_users, n_items))\n",
    "for line in test_data.itertuples():\n",
    "    test_data_matrix[line[1]-1, line[2]-1] = line[3]\n",
    "\n",
    "###\n",
    "for row in range (0, len(train_data_matrix)):\n",
    "    b = train_data_matrix[row]\n",
    "    m = np.mean(b[b>0])\n",
    "    for col in range(0, len(b)):\n",
    "        if b[col] == 0:\n",
    "            train_data_matrix[row,col] = m\n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "user_similarity = pairwise_distances(train_data_matrix, metric='cosine')\n",
    "item_similarity = pairwise_distances(train_data_matrix.T, metric='cosine')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "item_prediction = predict(train_data_matrix, item_similarity, type='item')\n",
    "user_prediction = predict(train_data_matrix, user_similarity, type='user')\n",
    "\n",
    "\n",
    "print 'User-based CF RMSE: ' + str(rmse(user_prediction, test_data_matrix))\n",
    "print 'Item-based CF RMSE: ' + str(rmse(item_prediction, test_data_matrix))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
